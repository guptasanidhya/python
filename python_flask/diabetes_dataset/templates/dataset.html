<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>{{ title }}</title>
    <link rel="stylesheet" href="../static/css/dataset.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
</head>
<body>
    <nav id="navbar">
        <ul>
            <div id="logo">
                <img src="../static\assets\img\navlogo.png" alt="logo">
                Diabetes Diagnoser
            </div>

            <li><a href="/">Home</a></li>
            <li><a href="/dataset">Pima's Dataset</a></li>
            <li><a href="/aboutus">About Us</a></li>
        </ul>
    </nav>

    <div class="container">
        <h1>End-to-End Data Science Example: Predicting Diabetes with Logistic Regression</h1>
        <div>
            <p>As the title suggests, this tutorial is an end-to-end example of solving a real-world problem using Data
                Science. We’ll be using Machine Learning to predict whether a person has diabetes or not, based on
                information about the patient such as blood pressure, body mass index (BMI), age, etc. The tutorial
                walks through the various stages of the data science workflow. In particular, the tutorial has the
                following sections
            </p>
            <ul>
                <li>Overview</li>
                <li>Data Description</li>
                <li>Data Exploration</li>
                <li>Data Preparation</li>
                <li>Training and Evaluating the Machine Learning Model </li>
                <li>Interpreting the ML Model</li>
                <li>Saving the model</li>
                <li>Making predictions</li>
            </ul>
            <img src="../static\assets\img\dataset_diabetes.jpg" alt="diabetes">
            <h2>Overview</h2>
            <p>The data was collected and made available by “National Institute of Diabetes and Digestive and Kidney
                Diseases” as part of the Pima Indians Diabetes Database. Several constraints were placed on the
                selection of these instances from a larger database. In particular, all patients here belong to the Pima
                Indian heritage (subgroup of Native Americans), and are females of ages 21 and above.</p>
            <p>We’ll be using Python and some of its popular data science related packages. First of all, we will import
                pandas to read our data from a CSV file and manipulate it for further use. We will also use numpy to
                convert out data into a format suitable to feed our classification model. We’ll use seaborn and
                matplotlib for visualizations. We will then import Logistic Regression algorithm from sklearn. This
                algorithm will help us build our classification model. Lastly, we will use pickle to save our model for
                future use.</p>
            <pre>
    from sklearn.linear_model import LogisticRegression
    import numpy as np
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    from sklearn.impute import SimpleImputer
    from sklearn.pipeline import Pipeline
    import pickle</pre>
            <h2>Data Description</h2>
            <p>We have our data saved in a CSV file called diabetes.csv. We first read our dataset into a pandas
                dataframe called diabetesDF, and then use the head() function to show the first five records from our
                dataset.</p>
            <pre>df = pd.read_csv('diabetes.csv')
        print(df.head())</pre>
            <img src="../static\assets\img\df_head.png" alt="df_head">
            <ul>The following features have been provided to help us predict whether a person is diabetic or not:
                <li>Pregnancies: Number of times pregnant</li>
                <li>Glucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test</li>
                <li>BloodPressure: Diastolic blood pressure (mm Hg)</li>
                <li>Skin SkinThickness: Triceps skin fold thickness (mm)</li>
                <li>Insulin:2-Hour serum insulin (mu U/ml)</li>
                <li>BMI: Body mass index (weight in kg/(height in m)2)</li>
                <li>DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes
                    based on family history)</li>
                <li>Age: Age (years)</li>
                <li>Class variable (0 if non-diabetic, 1 if diabetic)</li>
            </ul>
            <p>Let’s also make sure that our data is clean (has no null values, etc).</p>
            <pre>
df.info()
class 'pandas.core.frame.DataFrame'
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   pregnancies  768 non-null    int64  
 1   glucose      768 non-null    int64  
 2   diastolic    768 non-null    int64  
 3   triceps      768 non-null    int64  
 4   insulin      768 non-null    int64  
 5   bmi          768 non-null    float64
 6   dpf          768 non-null    float64
 7   age          768 non-null    int64  
 8   diabetes     768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB
 </pre>
            <p>Note that the data does have some missing values (see Insulin = 0) in the samples in the previous figure.
                Ideally we could replace these 0 values with the mode value for that feature, but we’ll skip that for
                now.
            </p>
            <h2>Data Exploration</h2>
            <p>Let’s start by finding correlation of every pair of features (and the outcome variable), and visualize
                the correlations using a heatmap.</p>
            <pre>
corr = df.corr()
print(corr)
sns.heatmap(corr,
xticklabels=corr.columns,
yticklabels=corr.columns)
plt.show()
    </pre>
            <img src="../static\assets\img\df_corr.png" alt="df_corr">
            <img src="../static\assets\img\sns_heatmap.png" alt="heatmap">
            <p>In the above heatmap, brighter colors indicate more correlation. As we can see from the table and the
                heatmap, glucose levels, age, BMI and number of pregnancies all have significant correlation with the
                outcome variable. Also notice the correlation between pairs of features, like age and pregnancies, or
                insulin and skin thickness.</p>
            <p>Let’s also look at how many people in the dataset are diabetic and how many are not. Below is the barplot
                of the same:
            </p>
            <img src="../static\assets\img\outcome.png" alt="outcomes">
            <img src="../static\assets\img\outcomeAge.png" alt="outcomes by age">
            <h3>Dataset Preparation (splitting and normalization)</h3>
            <p>When using machine learning algorithms we should always split our data into a training set and test set.
                (If the number of experiments we are running is large, then we can should be dividing our data into 3
                parts, namely — training set, development set and test set). In our case, we will also separate out some
                data for manual cross checking.</p>
            <p>The data set consists of record of 767 patients in total. To train our model we will be using 70%
                records. We will be using 30% records for testing.</p>
            <pre>
from sklearn.model_selection import train_test_split
X=df.iloc[:,df.columns!='diabetes']
 y=df['diabetes']
</pre>
            <p>Next, we separate the label and features (for both training and test dataset). In addition to that, we
                will also convert them into NumPy arrays as our machine learning algorithm process data in NumPy array
                format.
            </p>
            <pre>
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,stratify=df['diabetes'],random_state=42)
</pre>
            <p>As the final step before using machine learning, we will normalize our inputs. Machine Learning models
                often benefit substantially from input normalization. It also makes it easier for us to understand the
                importance of each feature later, when we’ll be looking at the model weights. We’ll normalize the data
                such that each variable has 0 mean and standard deviation of 1.</p>
            <pre>
means = np.mean(trainData, axis=0)
stds = np.std(trainData, axis=0)
trainData = (trainData - means)/stds
testData = (testData - means)/stds
# np.mean(trainData, axis=0) => check that new means equal 0
# np.std(trainData, axis=0) => check that new stds equal 1
                </pre>
            <h2>Training and Evaluating Machine Learning Model</h2>
            <p>We can now train our classification model. We’ll be using a machine simple learning model called logistic
                regression. Since the model is readily available in sklearn, the training process is quite easy and we
                can do it in few lines of code. First, we create an instance called logreg and then use the fit function
                to train the model.</p>
            <pre>

logreg = LogisticRegression()
logreg.fit(X_train,y_train)
y_pred = logreg.predict(X_test)
                </pre>
            <p>Next, we will use our test data to find out accuracy of the model. Using classification Model and
                Confusion Matrix</p>
            <pre>from sklearn.metrics import confusion_matrix
                    from sklearn.metrics import classification_report
                    print(confusion_matrix(y_test, y_pred))
                    print(classification_report(y_test, y_pred))
                </pre>
                <img src="../static\assets\img\accuracy.jpg" alt="accuracy">
            <p>The print statement will print confusion_matrix,precision,recall,f-score</p>
            <h2>Interpreting the ML Model</h2>
            <p>To get a better sense of what is going on inside the logistic regression model, we can visualize how our
                model uses the different features and which features have greater effect.</p>
            <pre>
coeff = list(logreg.coef_[0])
labels = list(X_train)
features = pd.DataFrame()
features['Features'] = labels
features['importance'] = coeff
features.sort_values(by=['importance'], ascending=True, inplace=True)
features['positive'] = features['importance'] > 0
features.set_index('Features', inplace=True)
features.importance.plot(kind='barh', figsize=(11, 6),color = features.positive.map({True: 'blue', False: 'red'}))
plt.xlabel('Importance')
plt.show()
                </pre>
                <p>Note that this above interpretations require that our input data is normalized. Without that, we can’t claim that importance is proportional to weights</p>
            <img src="../static\assets\img\importance.png" alt="importance">
<p>From the above figure, we can draw the following conclusions.
            <ol type="1">
                <li>Glucose level, BMI, pregnancies and diabetes pedigree function have significant influence on the
                    model, specially glucose level and BMI. It is good to see our machine learning model match what we
                    have been hearing from doctors our entire lives!</li>
                <li>Blood pressure has a negative influence on the prediction, i.e. higher blood pressure is correlated
                    with a person not being diabetic. (also, note that blood pressure is more important as a feature
                    than age, because the magnitude is higher for blood pressure).</li>
                <li>Although age was more correlated than BMI to the output variables (as we saw during data
                    exploration), the model relies more on BMI. This can happen for several reasons, including the fact
                    that the correlation captured by age is also captured by some other variable, whereas the
                    information captured by BMI is not captured by other variables.
                    </li>
            </ol>
            </p>
            <h2>Saving the Model</h2>
            <p>Now we will save our trained model for future use using pickle file</p>
            <pre>pickle.dump(logreg, open('model.pkl','wb'))

                        model = pickle.load(open('model.pkl','rb'))</pre>
        </div>
    </div>
    <footer id="footer">
        | Copyright 2022 learningpath.com | All rights reserved</footer>
</body>

</html>